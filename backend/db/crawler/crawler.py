import requests
import re 
from bs4 import BeautifulSoup as bs
import os
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from abc import ABC, abstractmethod
from time import sleep
import boto3
from fastapi import FastAPI, APIRouter, Depends, Query
from database.mongodb_connection import MongoDB
import shutil
import stat
import tempfile
import logging
import json
import io
from dotenv import load_dotenv

load_dotenv()
aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
aws_secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
aws_region = os.getenv("AWS_REGION")
aws_bucket_name = os.getenv("AWS_S3_BUCKET_NAME")

class BaseCrawler(ABC):
    def __init__(self, output_dir: str, place=""):
        self.output_dir = output_dir
        self.url = ""
        self.driver = None
        self.name = None

    def remove_readonly(self, func, path, _):
        """ì½ê¸° ì „ìš© íŒŒì¼ ì‚­ì œë¥¼ ìœ„í•œ ê¶Œí•œ ë³€ê²½"""
        os.chmod(path, stat.S_IWRITE)
        func(path)

    def send_to_db(self, data, db: "MongoDB"):  # âœ… ë¬¸ìžì—´ íƒ€ìž… ížŒíŒ…
        logging.info(f"MongoDB ì €ìž¥ ì‹œìž‘, ì €ìž¥í•  ë°ì´í„° ê°œìˆ˜: {len(data)}")

        inserted_ids = []
        logging.info(data)
        for property_data in data:
            logging.info(property_data)
            try:
                if isinstance(property_data, str):  # âœ… JSON ë¬¸ìžì—´ì´ë©´ ë³€í™˜
                    property_data = json.loads(property_data)

                if not isinstance(property_data, dict):  # âœ… ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê±´ë„ˆëœ€
                    logging.error(f"MongoDB ì €ìž¥ ì‹¤íŒ¨: ì˜ˆìƒ=dict, ì‹¤ì œ={type(property_data)}")
                    continue

                property_id = db.add_property(property_data, self.name)
                logging.info(f"MongoDB ì €ìž¥ ì™„ë£Œ: {property_id}")
                inserted_ids.append(property_id)
            except Exception as e:
                logging.error(f"MongoDB ì €ìž¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

        logging.info(f"MongoDB ì €ìž¥ ì™„ë£Œ, ì´ {len(inserted_ids)}ê°œ í•­ëª©ì´ ì¶”ê°€ë¨")

    def start_browser(self) -> None:
        """ì•ˆì „í•œ user-data-dirì„ ì‚¬ìš©í•˜ì—¬ Chrome ë¸Œë¼ìš°ì € ì‹¤í–‰"""
        # ê¸°ì¡´ ì‹¤í–‰ ì¤‘ì¸ Chrome í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
        running_processes = os.popen("pgrep -f chrome").read().strip()
        if running_processes:
            os.system("pkill -f chrome")

        options = webdriver.ChromeOptions()
        options.binary_location = "/opt/chrome/chrome-linux64/chrome"  # Chrome ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--remote-debugging-port=9222")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36") 

        # í”„ë¡œì„¸ìŠ¤ë³„ ê³ ìœ  user-data-dir ìƒì„±
        user_data_dir = tempfile.mkdtemp()
        if os.path.exists(user_data_dir):
            shutil.rmtree(user_data_dir, onerror=self.remove_readonly)
        options.add_argument(f"--user-data-dir={user_data_dir}")

        # Chromedriver ê²½ë¡œ ì§€ì •
        service = Service("/usr/local/bin/chromedriver")
        self.driver = webdriver.Chrome(service=service, options=options)
        self.driver.set_window_size(1920, 1080)
        self.driver.get(self.url)
        # ì´ˆê¸° íŽ˜ì´ì§€ ë¡œë“œë¥¼ ìœ„í•´ ìž ê¹ ëŒ€ê¸°(ë˜ëŠ” íŠ¹ì • ìš”ì†Œ ëŒ€ê¸°ë¡œ ëŒ€ì²´ ê°€ëŠ¥)
        WebDriverWait(self.driver, 10).until(lambda d: d.execute_script('return document.readyState') == 'complete')

    @abstractmethod
    def scrape_reviews(self):
        pass

    def save_to_database(self) -> None:
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ìž¥"""
        with open(os.path.join(self.output_dir, "data.json"), 'w') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=4)


class ThreeThreeCrawler(BaseCrawler):
    def __init__(self, output_dir, place="ì„œëŒ€ë¬¸êµ¬"):
        self.output_dir = os.path.join(output_dir, "threethree")
        os.makedirs(self.output_dir, exist_ok=True)
        self.data = []
        self.url = 'https://33m2.co.kr/webpc/search/keyword?keyword=ì„œëŒ€ë¬¸êµ¬&start_date=&end_date=&week='
        self.place = place
        self.name = "ë‹¨ê¸°ìž„ëŒ€"

    def scrape_reviews(self):
        driver = self.driver
        driver.get(self.url)
        sleep(3)

        data = []
        page = 1
        flag = True

        while flag:
            try:
                # ðŸ“Œ í˜„ìž¬ íŽ˜ì´ì§€ì—ì„œ ë°© ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                room_elements = driver.find_elements(By.CLASS_NAME, "room_item")

                for index, room in enumerate(room_elements):
                    try:
                        # âœ… ë°© í´ë¦­
                        room.find_element(By.XPATH, "./ancestor::a").click()
                        sleep(3)
                        
                        # âœ… ìƒˆ ì°½ìœ¼ë¡œ ì´ë™
                        windows = driver.window_handles
                        if len(windows) > 1:
                            driver.switch_to.window(windows[1])

                        # âœ… HTML íŒŒì‹±
                        soup = bs(driver.page_source, 'html.parser')
                        room_data = {}

                        # ðŸ“Œ 1ï¸âƒ£ ë°© ì œëª© í¬ë¡¤ë§
                        title_selector = 'body > div.wrap > section > div > div.room_detail > div.room_info > div.title > strong'
                        title = soup.select_one(title_selector)
                        room_data['title'] = title.text.strip() if title else "unknown"

                        # ðŸ“Œ 2ï¸âƒ£ ì´ë¯¸ì§€ í¬ë¡¤ë§ ë° S3 ì—…ë¡œë“œ
                        try:
                            s3 = boto3.client(
                                's3', aws_access_key_id=aws_access_key,
                                aws_secret_access_key=aws_secret_key,
                                region_name=aws_region,
                            )
                            bucket_name = "uni-rent-bucket"
                            img_name = room_data['title']
                            if img_name:
                                parent_div = driver.find_element(By.CLASS_NAME, 'swiper-wrapper')
                                images = parent_div.find_elements(By.TAG_NAME, "img")
                                for idx, img in enumerate(images):
                                    img_url = img.get_attribute("src")
                                    if img_url:
                                        try:
                                            img_response = requests.get(img_url)
                                            if img_response.status_code == 200:
                                                img_data = img_response.content
                                                sleep(2)
                                                s3.upload_fileobj(
                                                    io.BytesIO(img_data), bucket_name, f'{img_name}/{idx}.jpg'
                                                )
                                                print(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: {img_url}")
                                        except Exception as e:
                                            print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {img_url}, ì˜¤ë¥˜: {e}")
                        except Exception as e:
                            print(f"âŒ Swiper ì´ë¯¸ì§€ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

                        # ðŸ“Œ 3ï¸âƒ£ ì£¼ì†Œ í¬ë¡¤ë§
                        addr_selector = 'body > div.wrap > section > div > div.room_detail > div:nth-child(1) > p'
                        addr = soup.select_one(addr_selector)
                        room_data['addr'] = addr.text.strip() if addr else ""

                        # ðŸ“Œ 4ï¸âƒ£ ê°€ê²© í¬ë¡¤ë§
                        price_selector = 'body > div.wrap > section > div > div.room_sticky > div.room_pay > p > strong'
                        price = soup.select_one(price_selector)
                        room_data['price'] = price.text.strip() if price else ""

                        # ðŸ“Œ 5ï¸âƒ£ ì˜µì…˜ í¬ë¡¤ë§
                        options = []
                        option_selectors = [
                            'body > div.wrap > section > div > div.room_detail > div:nth-child(5) > ul',
                            'body > div.wrap > section > div > div.room_detail > div:nth-child(6) > ul'
                        ]
                        for selector in option_selectors:
                            option_elements = soup.select(selector)
                            for option in option_elements:
                                options.extend([p.text.strip() for p in option.find_all('p')])
                        room_data['options'] = options

                        # ðŸ“Œ 6ï¸âƒ£ ë°© ìƒì„¸ íŽ˜ì´ì§€ URL ì €ìž¥
                        room_data['url'] = driver.current_url
                        data.append(room_data)

                        # âœ… ì°½ ë‹«ê¸° ë° ì›ëž˜ ì°½ìœ¼ë¡œ ë³µê·€
                        driver.close()
                        driver.switch_to.window(windows[0])
                        sleep(2)

                    except Exception as e:
                        print(f"âŒ ë°© í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

                # ðŸ“Œ 7ï¸âƒ£ ë‹¤ìŒ íŽ˜ì´ì§€ë¡œ ì´ë™
                try:
                    next_page_button = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CLASS_NAME, 'next'))
                    )
                    next_page_button.click()
                    print("âœ… ë‹¤ìŒ íŽ˜ì´ì§€ ì´ë™ ì„±ê³µ")
                    sleep(3)

                except Exception as e:
                    print("âŒ ë‹¤ìŒ íŽ˜ì´ì§€ ì—†ìŒ. í¬ë¡¤ë§ ì¢…ë£Œ")
                    flag = False

            except Exception as e:
                print("âŒ íŽ˜ì´ì§€ í¬ë¡¤ë§ ì˜¤ë¥˜:", e)
                flag = False

        self.data = data

        # âœ… í¬ë¡¤ë§ ì™„ë£Œ í›„ ë“œë¼ì´ë²„ ì¢…ë£Œ
        driver.quit()


    def search_titles(self):
        """ë§¤ë¬¼ ëª©ë¡ì—ì„œ íƒ€ì´í‹€(title)ë§Œ ìˆ˜ì§‘ (ë°© ëª©ë¡ í´ë¦­ ë°©ì‹)"""
        self.start_browser()
        driver = self.driver
        driver.get(self.url)
        sleep(3)
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.ID, "div_search_result_inner")))
        
        data = []
        flag = True

        while flag:
            try:
                # ðŸ“Œ í˜„ìž¬ íŽ˜ì´ì§€ì—ì„œ ë°© ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                room_elements = driver.find_elements(By.CLASS_NAME, "room_item")
                for room in room_elements:
                    try:
                        # âœ… ë°© í´ë¦­
                        room.find_element(By.XPATH, "./ancestor::a").click()
                        sleep(3)
                        
                        # âœ… ìƒˆ ì°½ìœ¼ë¡œ ì´ë™
                        windows = driver.window_handles
                        if len(windows) > 1:
                            driver.switch_to.window(windows[1])
                        
                        # âœ… ìƒì„¸ íŽ˜ì´ì§€ì—ì„œ ì œëª© ìš”ì†Œ ë¡œë“œ ëŒ€ê¸° í›„ HTML íŒŒì‹±
                        wait.until(EC.presence_of_element_located(
                            (By.CSS_SELECTOR, 'body > div.wrap > section > div > div.room_detail > div.room_info > div.title > strong')
                        ))
                        soup = bs(driver.page_source, 'html.parser')
                        title_selector = 'body > div.wrap > section > div > div.room_detail > div.room_info > div.title > strong'
                        title_elem = soup.select_one(title_selector)
                        title_text = title_elem.text.strip() if title_elem else ""
                        data.append({"title": title_text})
                        logging.info(f"Collected title: {title_text}")
                        
                        # âœ… ì°½ ë‹«ê¸° ë° ì›ëž˜ ì°½ìœ¼ë¡œ ë³µê·€
                        driver.close()
                        driver.switch_to.window(windows[0])
                        sleep(2)
                        
                    except Exception as e:
                        logging.error("Exception during room title extraction: %s", e, exc_info=True)
                        if len(driver.window_handles) > 1:
                            driver.switch_to.window(driver.window_handles[1])
                            driver.close()
                            driver.switch_to.window(driver.window_handles[0])
                # ðŸ“Œ ë‹¤ìŒ íŽ˜ì´ì§€ë¡œ ì´ë™
                try:
                    next_page_button = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CLASS_NAME, 'next'))
                    )
                    next_page_button.click()
                    sleep(3)
                except Exception as e:
                    logging.error("Next page exception: %s", e, exc_info=True)
                    flag = False
                    
            except Exception as e:
                logging.error("Exception during room listing extraction: %s", e, exc_info=True)
                flag = False

        self.data = data
        driver.quit()

    def scrape_review_by_title(self, target_title):
        """
        ì£¼ì–´ì§„ ì œëª©(target_title)ê³¼ ì¼ì¹˜í•˜ëŠ” ë°©ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìŠ¤í¬ëž©í•˜ì—¬
        {title, addr, price, options, url, region, type} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜.
        """
        # ë¸Œë¼ìš°ì € ì‹œìž‘ ë° ì´ˆê¸° íŽ˜ì´ì§€ ë¡œë“œ
        self.start_browser()
        driver = self.driver
        driver.get(self.url)
        sleep(3)
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.ID, "div_search_result_inner")))
        
        room_data = None
        found = False
        flag = True
        
        while flag and not found:
            # í˜„ìž¬ íŽ˜ì´ì§€ì—ì„œ ë°© ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            room_elements = driver.find_elements(By.CLASS_NAME, "room_item")
            for room in room_elements:
                try:
                    # ë°© ëª©ë¡ì—ì„œ í•´ë‹¹ í•­ëª© í´ë¦­
                    room.find_element(By.XPATH, "./ancestor::a").click()
                    sleep(3)
                    
                    # ìƒˆ ì°½(ìƒì„¸ íŽ˜ì´ì§€)ë¡œ ì „í™˜
                    windows = driver.window_handles
                    if len(windows) > 1:
                        driver.switch_to.window(windows[1])
                    
                    # ìƒì„¸ íŽ˜ì´ì§€ì˜ íƒ€ì´í‹€ ìš”ì†Œ ë¡œë“œ ëŒ€ê¸°
                    wait.until(EC.presence_of_element_located(
                        (By.CSS_SELECTOR, 'body > div.wrap > section > div > div.room_detail > div.room_info > div.title > strong')
                    ))
                    soup = bs(driver.page_source, 'html.parser')
                    title_selector = 'body > div.wrap > section > div > div.room_detail > div.room_info > div.title > strong'
                    title_elem = soup.select_one(title_selector)
                    detail_title = title_elem.text.strip() if title_elem else ""
                    
                    # ìž…ë ¥í•œ ì œëª©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if detail_title == target_title:
                        room_data = {}
                        room_data['title'] = detail_title
                        
                        # ì£¼ì†Œ ì¶”ì¶œ
                        addr_selector = 'body > div.wrap > section > div > div.room_detail > div:nth-child(1) > p'
                        addr_elem = soup.select_one(addr_selector)
                        room_data['addr'] = addr_elem.text.strip() if addr_elem else ""
                        
                        # ê°€ê²© ì¶”ì¶œ
                        price_selector = 'body > div.wrap > section > div > div.room_sticky > div.room_pay > p > strong'
                        price_elem = soup.select_one(price_selector)
                        room_data['price'] = price_elem.text.strip() if price_elem else ""
                        
                        # ì˜µì…˜ ì¶”ì¶œ (ë‘ ì˜ì—­ í†µí•©)
                        options = []
                        option_selectors = [
                            'body > div.wrap > section > div > div.room_detail > div:nth-child(5) > ul',
                            'body > div.wrap > section > div > div.room_detail > div:nth-child(6) > ul'
                        ]
                        for selector in option_selectors:
                            option_elem = soup.select_one(selector)
                            if option_elem:
                                options.extend([p.text.strip() for p in option_elem.find_all('p')])
                        room_data['options'] = options
                        
                        # ìƒì„¸ íŽ˜ì´ì§€ URL ì €ìž¥
                        room_data['url'] = driver.current_url
                        # region ë° type ì¶”ê°€ (í´ëž˜ìŠ¤ ì´ˆê¸°í™” ì‹œ ìž…ë ¥í•œ ê°’ ì‚¬ìš©)
                        def extract_region(addr: str) -> str:
                            """
                            ðŸ“Œ ì£¼ì†Œ(addr)ì—ì„œ ë™(æ´ž) ë‹¨ìœ„ ì§€ì—­ëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
                            ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ì„œëŒ€ë¬¸êµ¬ í™ì œë™ 00ë²ˆì§€" -> "í™ì œë™"
                            """
                            match = re.search(r'([ê°€-íž£]+ë™)', addr)
                            return match.group(1) if match else "ì•Œ ìˆ˜ ì—†ìŒ"  # ë™ ì´ë¦„ì´ ì—†ìœ¼ë©´ "ì•Œ ìˆ˜ ì—†ìŒ" ë°˜í™˜

                        # ðŸ“Œ 1ï¸âƒ£ 'region' í•„ë“œ ìžë™ ì¶”ê°€ (addrì—ì„œ ë™(æ´ž) ì¶”ì¶œ)
                        room["region"] = extract_region(room.get("addr", ""))
                        room_data['type'] = self.name
                        
                        found = True
                        # ìƒì„¸ íŽ˜ì´ì§€ ì°½ ë‹«ê³  ëª©ë¡ ì°½ìœ¼ë¡œ ë³µê·€
                        driver.close()
                        driver.switch_to.window(windows[0])
                        break
                    else:
                        # ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„¸ íŽ˜ì´ì§€ ì°½ ë‹«ê³  ëª©ë¡ ì°½ìœ¼ë¡œ ëŒì•„ê°
                        driver.close()
                        driver.switch_to.window(windows[0])
                        sleep(1)
                except Exception as e:
                    print(f"Error processing a room: {e}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ, ì—´ë¦° ì°½ì´ ìžˆë‹¤ë©´ ë‹«ê³  ì›ëž˜ ì°½ìœ¼ë¡œ ë³µê·€
                    if len(driver.window_handles) > 1:
                        driver.switch_to.window(driver.window_handles[1])
                        driver.close()
                        driver.switch_to.window(driver.window_handles[0])
            
            # í˜„ìž¬ íŽ˜ì´ì§€ì—ì„œ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ë‹¤ìŒ íŽ˜ì´ì§€ë¡œ ì´ë™ ì‹œë„
            if not found:
                try:
                    next_page_button = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CLASS_NAME, 'next'))
                    )
                    next_page_button.click()
                    sleep(3)
                    wait.until(EC.presence_of_element_located((By.ID, "div_search_result_inner")))
                except Exception as e:
                    print("No more pages or error navigating to next page:", e)
                    flag = False
        
        driver.quit()
        return room_data
    
    def delete_properties_by_titles(self, titles) -> int:
        """
        ì£¼ì–´ì§„ ì œëª© ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ëª¨ë“  ë§¤ë¬¼ì„ MongoDB ì»¬ë ‰ì…˜ì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.
        
        :param titles: ì‚­ì œí•  ë§¤ë¬¼ ì œëª©ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        :return: ì‚­ì œëœ ë¬¸ì„œ ê°œìˆ˜
        """
        if not titles:
            return 0

        result = self.properties.delete_many({"title": {"$in": titles}})
        return result.deleted_count





class HowBoutHereCrawler(BaseCrawler):
    def __init__(self, output_dir: str, place="ì„œëŒ€ë¬¸êµ¬"):
        self.output_dir = os.path.join(output_dir, "howbouthere")
        os.makedirs(self.output_dir, exist_ok=True)
        self.data = []
        self.url = (
            """https://www.yeogi.com/domestic-accommodations?
            keyword=%EC%84%9C%EC%9A%B8+%EC%84%9C%EB%8C%80%EB%AC%B8%EA%B5%AC&
            autoKeyword=%EC%84%9C%EC%9A%B8+%EC%84%9C%EB%8C%80%EB%AC%B8%EA%B5%AC&
            checkIn=2025-02-22&checkOut=2025-02-23&personal=2&freeForm=false"""
        )
        self.place = place
        self.name = "ëª¨í…”"

    def scrape_reviews(self):
        """ëª¨í…” ì‚¬ì´íŠ¸ì—ì„œ ë¦¬ë·°ë¥¼ ìŠ¤í¬ëž©"""
        self.start_browser()
        driver = self.driver
        driver.get(self.url)
        wait = WebDriverWait(driver, 10)
        # ë©”ì¸ ì»¨í…ì¸  ì˜ì—­ ë¡œë“œ ëŒ€ê¸°
        wait.until(EC.presence_of_element_located((By.ID, "__next")))
        # ì´ˆê¸° íŽ˜ì´ì§€ ì†ŒìŠ¤ ìŠ¤ëƒ…ìƒ· ë¡œê¹… (ë””ë²„ê¹…ìš©)
        logging.info("HowBoutHere - Initial page source snapshot: %s", driver.page_source[:500])
        
        s3 = boto3.client(
            's3', aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key,
            region_name=aws_region,
            )
        bucket_name = "uni-rent-bucket"
        data = []
        page = 1
        while True:
            i = 1
            while True:
                try:
                    # ê° í•­ëª© í´ë¦­ ì „ ëŒ€ê¸°
                    link = wait.until(EC.element_to_be_clickable(
                        (By.XPATH, f'//*[@id="__next"]/div/main/section/div[2]/a[{i}]')
                    ))
                    link.click()
                    sleep(3)
                    
                    wait.until(EC.number_of_windows_to_be(2))
                    windows = driver.window_handles
                    if len(windows) > 1:
                        driver.switch_to.window(windows[1])
                    
                    # ìƒì„¸ íŽ˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° í›„ ì†ŒìŠ¤ ìŠ¤ëƒ…ìƒ· ë¡œê¹…
                    wait.until(EC.presence_of_element_located(
                        (By.CSS_SELECTOR, '#overview > div.css-3fvoms > div.css-mn17j9 > div.css-hn31yc > div.css-1tn66r8 > h1')
                    ))
                    logging.info("HowBoutHere - Detail page source snapshot: %s", driver.page_source[:500])
                    
                    soup = bs(driver.page_source, 'html.parser')
                    room = {}
                    
                    title_selector = '#overview > div.css-3fvoms > div.css-mn17j9 > div.css-hn31yc > div.css-1tn66r8 > h1'
                    title = soup.select(title_selector)
                    room['title'] = title[0].text if title else ""
                    
                    # ì´ë¯¸ì§€ ì²˜ë¦¬
                    img_name = room['title']
                    if img_name:
                        parent_div = wait.until(EC.presence_of_element_located(
                            (By.XPATH, '//*[@id="overview"]/article/div[1]/ul')
                        ))
                        images = parent_div.find_elements(By.TAG_NAME, "img")
                        for index, img in enumerate(images):
                            img_url = img.get_attribute("src")
                            if img_url:
                                img_data = re.get(img_url).content
                                sleep(1)
                                s3.upload_fileobj(
                                    io.BytesIO(img_data), bucket_name, f'{img_name}/{index}.jpg'
                                )
                    
                    addr_selector = '#overview > div.css-3fvoms > div.css-y3ur5y > div.css-1insk2s > a > p'
                    addr = soup.select(addr_selector)
                    room['addr'] = addr[0].text if addr else ""
                    
                    # ê°€ê²© í…Œì´ë¸” ì²˜ë¦¬
                    price_table = {}
                    j = 1
                    while True:
                        try:
                            price_selector = (f'#room > div.css-g6g7mu > div:nth-child({j}) > '
                                              'div.css-gp2jfw > div.css-hn31yc > div.css-1a09zno > '
                                              'div:nth-child(2) > div.css-1rw2dq2 > div.css-zpds22 > '
                                              'div > div > div.css-1l31u4y > div > div > div.css-a34t1s')
                            price = soup.select(price_selector)[0].text
                            key_selector = (f'#room > div.css-g6g7mu > div:nth-child({j}) > '
                                            'div.css-gp2jfw > div.css-zjkjbb > div.css-1ywt6mt > div')
                            key = soup.select(key_selector)[0].text
                            price_table[key] = price
                            j += 1
                        except Exception:
                            break
                    room['price_table'] = price_table
                    
                    # ì˜µì…˜ ì²˜ë¦¬
                    options = []
                    j = 1
                    while True:
                        try:
                            option_selector = (f'#__next > div > main > section.css-g9w49m > '
                                               f'div.css-2nct5r > div.css-1nuurnu > div.css-1kglajm > '
                                               f'div:nth-child({j}) > div > span')
                            option = soup.select(option_selector)[0].text
                            options.append(option)
                            j += 1
                        except Exception:
                            break
                    room['options'] = options
                    room['url'] = driver.current_url
                    data.append(room)
                    
                    # ì°½ ë‹«ê¸° ë° ì›ëž˜ ì°½ ë³µê·€
                    if len(driver.window_handles) > 1:
                        driver.switch_to.window(driver.window_handles[1])
                        driver.close()
                        driver.switch_to.window(driver.window_handles[0])
                        wait.until(EC.presence_of_element_located((By.ID, "__next")))
                        sleep(3)
                    i += 1
                
                except Exception as e:
                    logging.error("Exception during motel crawling: %s", e, exc_info=True)
                    break

            try:
                next_index = page % 10 + 2
                page += 1
                next_button = wait.until(EC.element_to_be_clickable(
                    (By.XPATH, f'//*[@id="__next"]/div/main/section/div[2]/div/div/button[{next_index}]')
                ))
                next_button.click()
                wait.until(EC.presence_of_element_located((By.ID, "__next")))
                sleep(3)
            except Exception as e:
                logging.error("Next page exception (motel): %s", e, exc_info=True)
                break
        self.data = data

