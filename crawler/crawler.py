import requests
from bs4 import BeautifulSoup as bs
import os
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from abc import ABC, abstractmethod
from time import sleep
import boto3
from fastapi import FastAPI, APIRouter, Depends, Query
import re 
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
# from database.mongodb_connection import MongoDB


class BaseCrawler(ABC):
    def __init__(self, output_dir: str, place=""):
        self.output_dir = output_dir
        self.url = ""
        self.driver = None



    def start_browser(self) -> None:
        '''start chrome browser'''
        driver = webdriver.Chrome()
        driver.maximize_window()
        driver.get(self.url)
        sleep(3)
        self.driver = driver

    @abstractmethod
    def scrape_reviews(self):
        pass

    def save_to_database(self) -> None:
        """Save reviews."""
        with open(os.path.join(self.output_dir, "data.json"),  'w', encoding="utf-8") as f:
            json.dump(self.data, f, ensure_ascii=False, indent=4)


class ThreeThreeCrawler(BaseCrawler):
    def __init__(self, output_dir, place="ì„œëŒ€ë¬¸êµ¬"):
        self.output_dir = os.path.join(output_dir, "threethree")
        os.makedirs(self.output_dir, exist_ok=True)
        self.data = []
        
        options = Options()
        # options.add_argument("--headless")  # UI ì—†ì´ ì‹¤í–‰ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
        options.add_argument("--disable-gpu")
        
        self.driver = webdriver.Chrome(options=options)
        self.url = f'https://33m2.co.kr/webpc/search/keyword?keyword={place}&start_date=&end_date=&week='
        self.place = place

    def scrape_reviews(self):
        driver = self.driver
        driver.get(self.url)
        sleep(3)

        data = []
        page = 1
        flag = True

        while flag:
            try:
                # ğŸ“Œ í˜„ì¬ í˜ì´ì§€ì—ì„œ ë°© ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                room_elements = driver.find_elements(By.CLASS_NAME, "room_item")

                for index, room in enumerate(room_elements):
                    try:
                        # âœ… ë°© í´ë¦­
                        room.find_element(By.XPATH, "./ancestor::a").click()
                        sleep(3)
                        
                        # âœ… ìƒˆ ì°½ìœ¼ë¡œ ì´ë™
                        windows = driver.window_handles
                        if len(windows) > 1:
                            driver.switch_to.window(windows[1])

                        # âœ… HTML íŒŒì‹±
                        soup = bs(driver.page_source, 'html.parser')
                        room_data = {}

                        # ğŸ“Œ 1ï¸âƒ£ ë°© ì œëª© í¬ë¡¤ë§
                        title_selector = 'body > div.wrap > section > div > div.room_detail > div.room_info > div.title > strong'
                        title = soup.select_one(title_selector)
                        room_data['title'] = title.text.strip() if title else "unknown"

                        # ğŸ“Œ 2ï¸âƒ£ í´ë” ìƒì„± (íŠ¹ìˆ˜ ë¬¸ì ì œê±°)
                        safe_title = re.sub(r'[\/:*?"<>|]', '_', room_data['title'])
                        img_dir = os.path.join(self.output_dir, safe_title)
                        img_dir = os.path.normpath(img_dir)
                        os.makedirs(img_dir, exist_ok=True)

                        print(f"âœ… í´ë” ìƒì„± ì™„ë£Œ: {img_dir}")

                        # ğŸ“Œ 3ï¸âƒ£ ì´ë¯¸ì§€ í¬ë¡¤ë§ ë° ì €ì¥
                        try:
                            parent_div = driver.find_element(By.CLASS_NAME, 'swiper-wrapper')
                            images = parent_div.find_elements(By.TAG_NAME, "img")

                            image_urls = set()  # ì¤‘ë³µ ì œê±°

                            for img in images:
                                img_url = img.get_attribute("src")
                                if img_url:
                                    image_urls.add(img_url)

                            for idx, img_url in enumerate(image_urls):
                                try:
                                    response = requests.get(img_url, stream=True)
                                    img_ext = os.path.splitext(img_url.split("?")[0])[1] or ".jpg"
                                    img_path = os.path.join(img_dir, f"{idx}{img_ext}")
                                    img_path = os.path.normpath(img_path)

                                    if response.status_code == 200:
                                        with open(img_path, "wb") as file:
                                            for chunk in response.iter_content(1024):
                                                file.write(chunk)
                                        print(f"âœ… Downloaded: {img_url}")

                                except Exception as e:
                                    print(f"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_url}, ì˜¤ë¥˜: {e}")

                        except Exception as e:
                            print(f"âŒ Swiper ì´ë¯¸ì§€ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

                        # ğŸ“Œ 4ï¸âƒ£ ì£¼ì†Œ í¬ë¡¤ë§
                        addr_selector = 'body > div.wrap > section > div > div.room_detail > div:nth-child(1) > p'
                        addr = soup.select_one(addr_selector)
                        room_data['addr'] = addr.text.strip() if addr else ""

                        # ğŸ“Œ 5ï¸âƒ£ ê°€ê²© í¬ë¡¤ë§
                        price_selector = 'body > div.wrap > section > div > div.room_sticky > div.room_pay > p > strong'
                        price = soup.select_one(price_selector)
                        room_data['price'] = price.text.strip() if price else ""

                        # ğŸ“Œ 6ï¸âƒ£ ì˜µì…˜ í¬ë¡¤ë§
                        options = []
                        option_selectors = [
                            'body > div.wrap > section > div > div.room_detail > div:nth-child(5) > ul',
                            'body > div.wrap > section > div > div.room_detail > div:nth-child(6) > ul'
                        ]
                        for selector in option_selectors:
                            option_elements = soup.select(selector)
                            for option in option_elements:
                                options.extend([p.text.strip() for p in option.find_all('p')])

                        room_data['options'] = options

                        # ğŸ“Œ 7ï¸âƒ£ ë°© ìƒì„¸ í˜ì´ì§€ URL ì €ì¥
                        room_data['url'] = driver.current_url
                        data.append(room_data)

                        # âœ… ì°½ ë‹«ê¸° ë° ì›ë˜ ì°½ìœ¼ë¡œ ë³µê·€
                        driver.close()
                        driver.switch_to.window(windows[0])
                        sleep(2)

                    except Exception as e:
                        print(f"âŒ ë°© í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

                # ğŸ“Œ 8ï¸âƒ£ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                try:
                    next_page_button = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CLASS_NAME, 'next'))
                    )
                    next_page_button.click()
                    print("âœ… ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì„±ê³µ")
                    sleep(3)

                except Exception as e:
                    print("âŒ ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ. í¬ë¡¤ë§ ì¢…ë£Œ")
                    flag = False

            except Exception as e:
                print("âŒ í˜ì´ì§€ í¬ë¡¤ë§ ì˜¤ë¥˜:", e)
                flag = False

        self.data = data

        # âœ… í¬ë¡¤ë§ ì™„ë£Œ í›„ ë“œë¼ì´ë²„ ì¢…ë£Œ
        driver.quit()
    


                


class HowBoutHereCrawler(BaseCrawler):
    def __init__(self, output_dir: str, place="ì„œëŒ€ë¬¸êµ¬"):
        '''Constructor for JSCrawler'''
        self.output_dir = os.path.join(output_dir, "howbouthere")
        os.makedirs(self.output_dir, exist_ok=True)
        self.data = []
        options = Options()
        # options.add_argument("--headless")  # Run in headless mode (no UI)
        options.add_argument("--disable-gpu")  # Recommended for some systems
        self.driver: webdriver.Chrome = webdriver.Chrome(options=options)
        self.url = "https://www.yeogi.com/domestic-accommodations?keyword=%EC%84%9C%EC%9A%B8+%EC%84%9C%EB%8C%80%EB%AC%B8%EA%B5%AC&autoKeyword=%EC%84%9C%EC%9A%B8+%EC%84%9C%EB%8C%80%EB%AC%B8%EA%B5%AC&checkIn=2025-02-22&checkOut=2025-02-23&personal=2&freeForm=false"
        self.place = place
    
    def scrape_reviews(self):
        driver = self.driver
        url = self.url
        driver.get(url)
        sleep(3)

        data = []
        page = 1

        while True:
            i = 1
            while True:
                try:
                    driver.find_element(By.XPATH, f'//*[@id="__next"]/div/main/section/div[2]/a[{i}]').click()
                    sleep(2)
                    windows = driver.window_handles
                    if len(windows) > 1:
                        driver.switch_to.window(windows[1])
                    soup = bs(driver.page_source, 'html.parser')
                    room = {}
                    
                    # ìˆ™ì†Œ ì œëª© í¬ë¡¤ë§
                    title_selector = '#overview > div.css-3fvoms > div.css-mn17j9 > div.css-hn31yc > div.css-1tn66r8 > h1'
                    title = soup.select(title_selector)
                    room['title'] = title[0].text.strip() if title else "unknown"

                    # ğŸ“Œ íŠ¹ìˆ˜ ë¬¸ì ì œê±° í›„ í´ë”ëª… ë³€í™˜
                    safe_title = re.sub(r'[\/:*?"<>|]', '_', room['title'])
                    img_dir = os.path.join(self.output_dir, safe_title)
                    img_dir = os.path.normpath(img_dir)  # ìš´ì˜ì²´ì œë³„ ê²½ë¡œ ì •ë¦¬
                    os.makedirs(img_dir, exist_ok=True)
                    print(f"âœ… í´ë” ìƒì„± ì™„ë£Œ: {img_dir}")

                    # # ğŸ“Œ ì´ë¯¸ì§€ í¬ë¡¤ë§ ë° ì €ì¥
                    # try:
                    #     parent_div = driver.find_element(By.XPATH, '//*[@id="overview"]/article/div[1]/ul')
                    #     images = parent_div.find_elements(By.TAG_NAME, "img")

                    #     image_urls = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ Set

                    #     for img in images:
                    #         img_url = img.get_attribute("src")
                    #         if img_url and img_url not in image_urls:
                    #             image_urls.add(img_url)

                    #     # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
                    #     for index, img_url in enumerate(image_urls):
                    #         try:
                    #             headers = {
                    #                 "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
                    #             }

                    #             response = requests.get(img_url, headers=headers, stream=True)
                    #             print(f"ì‘ë‹µ ë°ì´í„° í¬ê¸°: {len(response.content)} ë°”ì´íŠ¸")

                    #             # ğŸ“Œ ì´ë¯¸ì§€ í™•ì¥ì ìë™ ì¶”ì¶œ
                    #             img_ext = os.path.splitext(img_url.split("?")[0])[1]
                    #             if not img_ext:  # í™•ì¥ìê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
                    #                 img_ext = ".jpg"

                    #             # ğŸ“Œ ì˜¬ë°”ë¥¸ ì €ì¥ ê²½ë¡œ ìƒì„±
                    #             img_path = os.path.join(img_dir, f"{index}{img_ext}")
                    #             img_path = os.path.normpath(img_path)  # ìš´ì˜ì²´ì œë³„ ê²½ë¡œ ì •ë¦¬

                    #             print(f"ì´ë¯¸ì§€ê°€ ì €ì¥ë  ê²½ë¡œ: {img_path}")

                    #             if response.status_code == 200:
                    #                 with open(img_path, "wb") as file:
                    #                     for chunk in response.iter_content(1024):
                    #                         file.write(chunk)
                    #                 print(f"âœ… Downloaded: {img_url}")

                    #         except Exception as e:
                    #             print(f"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_url}, ì˜¤ë¥˜: {e}")

                    # except Exception as e:
                    #     print(f"âŒ ì´ë¯¸ì§€ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

                    # ğŸ“Œ ì£¼ì†Œ í¬ë¡¤ë§
                    addr_selector = '#overview > div.css-3fvoms > div.css-y3ur5y > div.css-1insk2s > a > p'
                    addr = soup.select(addr_selector)
                    room['addr'] = addr[0].text.strip() if addr else ""

                    def extract_region(addr: str) -> str:
                        """
                        ğŸ“Œ ì£¼ì†Œ(addr)ì—ì„œ ë™(æ´) ë‹¨ìœ„ ì§€ì—­ëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
                        ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ì„œëŒ€ë¬¸êµ¬ í™ì œë™ 00ë²ˆì§€" -> "í™ì œë™"
                        """
                        match = re.search(r'([ê°€-í£]+ë™)', addr)
                        return match.group(1) if match else "ì•Œ ìˆ˜ ì—†ìŒ"  # ë™ ì´ë¦„ì´ ì—†ìœ¼ë©´ "ì•Œ ìˆ˜ ì—†ìŒ" ë°˜í™˜

                    # ğŸ“Œ 1ï¸âƒ£ 'region' í•„ë“œ ìë™ ì¶”ê°€ (addrì—ì„œ ë™(æ´) ì¶”ì¶œ)
                    room["region"] = extract_region(room.get("addr", ""))

                    # ğŸ“Œ 2ï¸âƒ£ ìˆ™ì†Œ ìœ í˜•(type) ì¶”ê°€ ("ëª¨í…”"ë¡œ ê³ ì •)
                    room["type"] = "ëª¨í…”"

                    # ğŸ“Œ ê°€ê²© í¬ë¡¤ë§ ë° ìµœì €ê°€ ê³„ì‚°
                    j = 1
                    price_table = {}
                    min_price = float('inf')  # ìµœì €ê°€ ì°¾ê¸° ìœ„í•œ ì´ˆê¸°ê°’

                    while True:
                        try:
                            # ğŸ“Œ ê°€ê²© ì •ë³´ í¬ë¡¤ë§
                            price_selector = f'#room > div.css-g6g7mu > div:nth-child({j}) > div.css-gp2jfw > div.css-hn31yc > div.css-1a09zno > div:nth-child(2) > div.css-1rw2dq2 > div.css-zpds22 > div > div > div.css-1l31u4y > div > div > div.css-a34t1s'
                            key_selector = f'#room > div.css-g6g7mu > div:nth-child({j}) > div.css-gp2jfw > div.css-zjkjbb > div.css-1ywt6mt > div'

                            price_text = soup.select(price_selector)[0].text.strip()
                            key_text = soup.select(key_selector)[0].text.strip()

                            # ğŸ“Œ ê°€ê²© ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: "â‚©120,000" â†’ "120000")
                            price_numeric = int(re.sub(r"[^\d]", "", price_text))

                            # ğŸ“Œ ê°€ê²© í…Œì´ë¸”ì— ì €ì¥
                            price_table[key_text] = price_numeric

                            # ğŸ“Œ ìµœì €ê°€ ì—…ë°ì´íŠ¸
                            min_price = min(min_price, price_numeric)

                            j += 1  # ë‹¤ìŒ ê°€ê²© í•­ëª©ìœ¼ë¡œ ì´ë™

                        except:
                            break  # ë” ì´ìƒ ê°€ê²© ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ

                    # ğŸ“Œ ìµœì €ê°€ ê¸°ë°˜ 1ì£¼ì¼ ê°€ê²© ê³„ì‚° (ìµœì €ê°€ * 7)
                    room['price_table'] = price_table
                    room['price'] = min_price * 7 if min_price != float('inf') else None  # ê°€ê²©ì´ ì—†ìœ¼ë©´ None ì €ì¥

                    # ğŸ“Œ ë°ì´í„° ì €ì¥
                    room['url'] = driver.current_url
                    data.append(room)

                    # ğŸ“Œ ì°½ ë‹«ê¸° ë° ì›ë˜ ì°½ìœ¼ë¡œ ë³µê·€
                    windows = driver.window_handles
                    if len(windows) > 1:
                        driver.switch_to.window(windows[1])
                        driver.close()
                        driver.switch_to.window(windows[0])
                        sleep(2)
                    
                    i += 1
                
                except Exception as e:
                    print(e)
                    break

            # ğŸ“Œ ë‹¤ìŒ í˜ì´ì§€ ì´ë™
            try:
                next_page = page % 10 + 2
                page += 1
                driver.find_element(By.XPATH, f'//*[@id="__next"]/div/main/section/div[2]/div/div/button[{next_page}]').click()
                sleep(3)
                
            except:
                break
        
        self.data = data
